{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8cd2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beb9598",
   "metadata": {},
   "source": [
    "Recall that in this module we're sticking with a given set of versions of libraries (such as `pandas`) which match the Anaconda installation available in the IT labs.  It is a convention in Python libraries that the version string of the library is available via the `__version__` attribute of the module.\n",
    "\n",
    "In this notebook I'll include links to the documentation for various function calls corresponding to the version of `pandas` we're using.  The `pandas` documentation site maintains versions of the documentation for all (recent) releases - you'll always want to check that you're consulting the documentation for the version you're using, as libraries do evolve over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b23033",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6093f8b1",
   "metadata": {},
   "source": [
    "In the experiment, we collected the data in two batches.  These are the folders `batch1` and `batch2` in `data/raw`.  Each folder has files with the same names, and the files with the same names have the same **schema** - that is to say, they have the same set of column names, and each column name means the same thing in the two batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b233d528",
   "metadata": {},
   "outputs": [],
   "source": [
    "decisions1 = pd.read_csv(\"data/raw/batch1/decisions.csv\")\n",
    "decisions1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3757e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "decisions2 = pd.read_csv(\"data/raw/batch2/decisions.csv\")\n",
    "decisions2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1452779",
   "metadata": {},
   "source": [
    "Well, so far so good; there is the same number of columns across the two `decisions` files, and visually scanning the tables suggests they do have the same schemas.  But let's check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f9067a",
   "metadata": {},
   "outputs": [],
   "source": [
    "decisions1.columns == decisions2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd44dda",
   "metadata": {},
   "source": [
    "File formats such as CSV do not have any way to represent the types of the data it contains - that is, whether the data are integers, floating-point numbers, text strings, dates, and so on.  `read_csv` [https://pandas.pydata.org/pandas-docs/version/1.2.4/reference/api/pandas.read_csv.html] attempts to *infer* the type of data by inspecting the contents of the file.  This often works well if your input data are well-behaved.  Because today we are working with data which \"we\" generated in an experiment where we wrote the program, our data will tend to be tidy.  But again we will check to be sure!\n",
    "\n",
    "If you have a `DataFrame`, you can get a list of the data types using the `dtypes` attribute [https://pandas.pydata.org/pandas-docs/version/1.2.4/reference/api/pandas.DataFrame.dtypes.html]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eea329a",
   "metadata": {},
   "outputs": [],
   "source": [
    "decisions1.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fbc97f",
   "metadata": {},
   "source": [
    "**Fun fact**: `dtypes` is itself a `pandas` `Series`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91037afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(decisions1.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae565aa",
   "metadata": {},
   "source": [
    "So all of the things you've learned about working with `Series` apply - including that you can get the data type for a column via using the square-brackets notation like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d83617",
   "metadata": {},
   "outputs": [],
   "source": [
    "decisions1.dtypes['player.lotterychoice']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65628daa",
   "metadata": {},
   "source": [
    "This also means that we can check for the equality of the `dtypes` of our two `DataFrame`s by using the `==` operator.  Recall that this operator works by comparing entries with the same index label, so you get out another `Series` which shows the result of the comparison for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad24788",
   "metadata": {},
   "outputs": [],
   "source": [
    "decisions1.dtypes == decisions2.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f577fa75",
   "metadata": {},
   "source": [
    "We want it to be the case that all of our columns have the same datatype - we can do this using the `all()` method on the `Series`. [https://pandas.pydata.org/pandas-docs/version/1.2.4/reference/api/pandas.Series.all.html]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b113272",
   "metadata": {},
   "outputs": [],
   "source": [
    "(decisions1.dtypes == decisions2.dtypes).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3605bd",
   "metadata": {},
   "source": [
    "At this point we feel confident enough our two `DataFrames` do have the same schema, so we can go ahead and concatenate them using `concat` [https://pandas.pydata.org/pandas-docs/version/1.2.4/reference/api/pandas.concat.html].\n",
    "\n",
    "The index of our two `DataFrame`s was simply an auto-generated row number when we read the files in.  These don't have any meaning to use, so we use the `ignore_index` parameter to tell `pandas` that after concatenating the data, create a new indexing simply labeling each observation by an (arbitrary) row number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e02cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_decisions = pd.concat([decisions1, decisions2], ignore_index=True)\n",
    "raw_decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e32a6c",
   "metadata": {},
   "source": [
    "As the name of the folder suggests, the data we have in `raw` has come directly from the source - in our case, the server on which the experiment was hosted.  The files we have stored here are **completely untouched**.\n",
    "\n",
    "This illustrates one of the key principles of data science projects:\n",
    "\n",
    "**RAW (SOURCE) DATA IS IMMUTABLE**\n",
    "\n",
    "What this means is that we always keep a copy of the file(s) we start with.  Those files will rarely be in exactly the format we want.  But that's OK.\n",
    "\n",
    "What many (most) people are tempted to do (or, in fact, do!) is to start manipulating the file by, for example, loading it into Excel (shudder!), editing it, and then saving it back, often overwriting the original.  **You should never do this.**. Perhaps once upon a time, before there were great libraries like `pandas` (or the Tidyverse in R, or other similar libraries), cleaning and transforming data was difficult to do, and maybe manual editing was a practical if imperfect solution.\n",
    "\n",
    "But today there is no excuse!  A key learning objective of this module (basically, the entire reason it exists) is to give you the tools to manage, transform, and analyse data such that every step you take is completely reproducible by yourself, and by anyone else.  Especially in the next four lectures, we will develop good habits and practices for how to accomplish that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15757f9c",
   "metadata": {},
   "source": [
    "In the case of the dataset from our experiment, the experiment software keeps some standard data fields which are used only for some types of experiments.  These were not relevant for our experiment, and so they are null (blank) in all of our observations.  We can see which entries are null by calling `isnull` [https://pandas.pydata.org/pandas-docs/version/1.2.4/reference/api/pandas.DataFrame.isnull.html]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618df770",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_decisions.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a82e5e9",
   "metadata": {},
   "source": [
    "That gives us whether the individual cells are nulls - we want to know which columns are entirely null.  Just as before, we can use `all()` to aggregate the columns and report whether all of the entries are nulls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c13044",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_decisions.isnull().all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13c7bb7",
   "metadata": {},
   "source": [
    "Dropping columns which are totally null can be done with the `dropna()` method on a `DataFrame` [https://pandas.pydata.org/pandas-docs/version/1.2.4/reference/api/pandas.DataFrame.dropna.html].  We will use it here to drop columns - but you can also use the method to drop rows which are entirely null.  (We don't have any of those)\n",
    "\n",
    "`dropna` comes with a bit of a \"gotcha\" - its default mode is to drop a column if **any** of the entries are null.  In our case, we want to make sure to drop only when **all** of the entries are null.\n",
    "\n",
    "We'll assign the resulting `DataFrame` with the null columns dropped to a variable `df`.  The variable name `df` is commonly used by convention when you're working with just one `DataFrame` and don't need to distinguish among more than one - you will see it in a lot of examples in the `pandas` documentation and elsewhere.  There's nothing special about using `df` other than it's mnemonic and easy to type - you could use any variable name you wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5995f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_decisions.dropna(axis='columns', how='all')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3389bbb1",
   "metadata": {},
   "source": [
    "Although we have no rows where the data are all none, we do have some rows that we don't want to consider for analysis.  Here we are needing to apply our specialist knowledge of the dataset and how it was generated.\n",
    "\n",
    "First, the experimental software has a \"demo\" mode that is used for testing purposes.  Because part of testing is ensuring data is recorded correctly, the \"demo\" data also appears in this file.  However, we don't want to include it for data analysis.  So, we want to remove rows which are flagged as being demo.\n",
    "\n",
    "To select a subset of rows, the best way is to use the `query` method. [https://pandas.pydata.org/pandas-docs/version/1.2.4/reference/api/pandas.DataFrame.query.html]. This is a bit of an unusal method in that it takes a text string - and the text string is then a logical expression.  (There are practical reasons for this, which perhaps I'll have time to mention in due course...)\n",
    "\n",
    "We have one extra wrinkle here, which is that because we have a full-stop in the variable name, we need to surround the field name in the expression using back-ticks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feae466",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.query(\"`session.is_demo` == 0\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e77f1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.query(\"`player.lotterychoice`.notnull()\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796bc1ef",
   "metadata": {},
   "source": [
    "In our file, most of the columns are actually redundant for analysis purposes - they're there to support the infrastructure of running the experiment, or for diagnostic reasons (confirming the experiment software is doing what we want it to).  It's good for us to keep them in our `raw` data, but we won't need them for data analysis.\n",
    "\n",
    "We can shape our `DataFrame` to include the columns we want using `reindex()`.  [https://pandas.pydata.org/pandas-docs/version/1.2.4/reference/api/pandas.DataFrame.reindex.html]. We give `reindex` a list of column names, and it returns a `DataFrame` with the data just from those columns.  Notice that we also use it to re-order the columns here.  My own personal taste is to put \"metadata\" at the start, in decreasing order of scope.  So we have sessions, and in each session there is a participant, and each participant plays a number of rounds, hence the ordering of the first three columns.  (This is a personal preference and not a hard rule, but it is good to develop a convention and try to stick with it!  Operations like `reindex()` make this easy to do.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef22c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reindex(\n",
    "    columns=[\n",
    "        'session.label', 'participant.code', 'subsession.round_number', 'player.menu_number',\n",
    "        'player.displayed_first', 'player.lotterychoice'\n",
    "    ]\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e157dc68",
   "metadata": {},
   "source": [
    "OK, so earlier we saw that having full-stops in column names can be annoying, because when we ran `query()` we had to put extra characters in our query string.  Indeed, a lot of software does not like column names with periods, or spaces, or other characters in them.\n",
    "\n",
    "We can use the `rename` method on `DataFrame` to re-label columns.  [https://pandas.pydata.org/pandas-docs/version/1.2.4/reference/api/pandas.DataFrame.rename.html]\n",
    "\n",
    "There are a few ways you can use `rename`.  One way is to pass a function to the `columns` parameter.  This function is then called on each column name.  This function takes each column name, and replaces all instances of a full-stop with an underscore - usually, underscores are a safe character to have in your column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e213be39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns=lambda x: x.replace(\".\", \"_\"))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ddee35",
   "metadata": {},
   "source": [
    "Another way to use `rename` is to pass a `dict` which maps how to change existing column names into the ones you want.\n",
    "\n",
    "(In this case, we could have just done a `rename` like this one straightaway without doing the function-based one.  I wanted to show you that as well, because often you will want to do a mass-rename according to some pattern.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d056291",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\n",
    "    'session_label': 'session_id',\n",
    "    'participant_code': 'participant_id',\n",
    "    'subsession_round_number': 'round_id',\n",
    "    'player_menu_number': 'menu_id',\n",
    "    'player_displayed_first': 'displayed_first',\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5035f69",
   "metadata": {},
   "source": [
    "In the experiment, each menu consisted of two choices, which are labeled 'p' and 'q' for reasons having to do with the theory being tested.  To avoid ordering effects, the software randomised whether 'p' or 'q' was displayed first.  The choice recorded is then 0 or 1, where 0 means the subject choice the first lottery shown and 1 means they chose the second.\n",
    "\n",
    "When we analyse the data we don't want to have to worry about remembering that, so we transform the data so we just directly record the name of the lottery chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4781183",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.assign(\n",
    "     chose_p=lambda x: (\n",
    "        ((x['displayed_first'] == \"p\") & (x['player_lotterychoice'] == 0)) |\n",
    "        ((x['displayed_first'] == \"q\") & (x['player_lotterychoice'] == 1))\n",
    "     )\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403b7f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.assign(\n",
    "    choice=lambda x: x['chose_p'].replace({True: \"q\", False: \"p\"})\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe2742e",
   "metadata": {},
   "source": [
    "Now we close with some tidying up for human consumption.  Let's start by keeping just the columns we care about, discarding the interim columns we used in computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3122a435",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reindex(\n",
    "    columns=['session_id', 'participant_id', 'round_id', 'menu_id', 'displayed_first', 'choice']\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bc19c2",
   "metadata": {},
   "source": [
    "The menu IDs were actually integers.  However, `read_csv` imported them as floating-point numbers.  This has to do with how `pandas` deals with null values in numeric types - historically, it was not possible to have null values in integer fields, and so `read_csv` made the menu ID column to be floating-point because our original file had null entries for the menu ID.  Now that we have tidied things up, we can make the menu ID to be an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb0d0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.astype({'menu_id': int})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62d2444",
   "metadata": {},
   "source": [
    "And finally, let's order the data in the way that's most convenient for us.  In terms of the experiment, what we are interested in is the 25 choices of each participant, so it makes sense to order by participant.  Further, we're most interested in the choices by menu, and not necessarily the order in which the participant saw each of the menus.  So this is the most natural ordering.\n",
    "\n",
    "Sorting data like this is entirely cosmetic - it's just for our benefit as humans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a758ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(['session_id', 'participant_id', 'menu_id'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d6831c",
   "metadata": {},
   "source": [
    "If you were paying close attention, you'll note that I pulled a bit of a fast one on you.  Once I started working with the data, I was assigning the resulting `DataFrame` to a new variable `df`.  What this means is that actually the original data in `raw_decisions` is still there - and completely unchanged!\n",
    "\n",
    "That might not seem like something to make a big deal about.  However, this turns out to be **extremely powerful** in practice, for many reasons.  Some of these reasons are technical and have to do with how calculations on `DataFrame`s are implemented behind-the-scenes; you probably won't have to worry about those until you're a very advanced user.  Other reasons however will come up in our examples over the next few weeks.\n",
    "\n",
    "A way of thinking about how we work with data is that we do not **change** data, but instead we **transform** it.  Each step we did above had the same logical structure: You start with a `DataFrame`, you apply some operation to it, and then you get out another `DataFrame`.  Now, you might start to get worried by this.  We are working here with a quite small `DataFrame` - but when you're working on a project you might have a huge `DataFrame`.  Isn't it inefficient (both in memory and processing speed) to be creating a new `DataFrame` each time?  Well, it turns out that there are techniques which libraries can use to avoid copying data unless necessary - and so actually many of these operations can be implemented very efficiently.\n",
    "\n",
    "Efficiency aside, because of this property that every operation transforms one `DataFrame` into another `DataFrame`, we can do something **very cool**.  We can write all of the transformations we did on our data above as **one single Python expression**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961e9941",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = (\n",
    "    raw_decisions.dropna(axis='columns', how='all')\n",
    "    .query(\"`session.is_demo` == 0\")\n",
    "    .query(\"`player.lotterychoice`.notnull()\")\n",
    "    .reindex(\n",
    "        columns=[\n",
    "            'session.label', 'participant.code', 'subsession.round_number', 'player.menu_number',\n",
    "            'player.displayed_first', 'player.lotterychoice'\n",
    "        ]\n",
    "    )\n",
    "    .rename(columns=lambda x: x.replace(\".\", \"_\"))\n",
    "    .rename(columns={\n",
    "        'session_label': 'session_id',\n",
    "        'participant_code': 'participant_id',\n",
    "        'subsession_round_number': 'round_id',\n",
    "        'player_menu_number': 'menu_id',\n",
    "        'player_displayed_first': 'displayed_first',\n",
    "    })\n",
    "    .assign(\n",
    "        chose_p=lambda x: (\n",
    "            ((x['displayed_first'] == \"p\") & (x['player_lotterychoice'] == 0)) |\n",
    "            ((x['displayed_first'] == \"q\") & (x['player_lotterychoice'] == 1))\n",
    "        )\n",
    "    )\n",
    "    .assign(\n",
    "        choice=lambda x: x['chose_p'].replace({True: \"q\", False: \"p\"})\n",
    "    )\n",
    "    .reindex(\n",
    "        columns=['session_id', 'participant_id', 'round_id', 'menu_id', 'displayed_first', 'choice']\n",
    "    )\n",
    "    .astype({'menu_id': int})\n",
    "    .sort_values(['session_id', 'participant_id', 'menu_id'])\n",
    ")\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53fab1a",
   "metadata": {},
   "source": [
    "Is this new `df2` we have constructed this way identical to the `df` we built step-by-step above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaaac5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df == df2).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ec2f6c",
   "metadata": {},
   "source": [
    "Yes it is!\n",
    "\n",
    "Isn't that absolutely brilliant?\n",
    "\n",
    "On the one hand, that expression we used to make `df2` is a lot to take in, and when you first see that all at once it could be overwhelming.  However, look at each step individually, one-by-one: each line of code represents one logical step in the process.  So, when you write your code like this, you're also documenting the process you used to go from the raw data to the finished product.\n",
    "\n",
    "This style of programming is sometimes called a more \"declarative\" style, and comes from the world of what's called \"functional programming\".  The emphasis in declarative-style programming is on writing what task you want the computer to do, as opposed to micro-managing the implementation of how the task is carried out.  (The latter is sometimes called \"imperative programming\".)  When you use a library like `pandas`, you can interact with the data at a very high level of abstraction, and leave it to the library implementers to come up with efficient ways to carry out each of those operations.\n",
    "\n",
    "The style of chaining of the `DataFrame` methods used here is sometimes called a *fluent interface*.  If you are familiar with R (or someday use it), the `dplyr` library in R's `tidyverse` uses the \"%>%\" operator, which accomplishes basically the same thing.\n",
    "\n",
    "When we're working in Jupyter notebooks, we'll typically only take one step at a time because we're working interactively and want to see the output of each step.  So we won't too often chain all of our transformations together like the above.  In a few weeks' time we'll talk about how to organise your code into scripts, and there we will tend to use this chaining technique to very powerful effect!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5154fb6",
   "metadata": {},
   "source": [
    "Let's do a little more data checking and a bit of analysis.  First, we expect that each participant should have 25 choices recorded.\n",
    "\n",
    "To do this, we group our data by `participant_id`. [https://pandas.pydata.org/pandas-docs/version/1.2.4/reference/api/pandas.DataFrame.groupby.html].   The easiest way to think of what `groupby` does is that it creates a collection of sub-`DataFrame`s, one for each of the values that we have grouped by.\n",
    "\n",
    "Then, we can do operations on each of those individual sub-`DataFrame`s.  Here, we will use the `count` method, which gives the number of non-null values in each column. [https://pandas.pydata.org/pandas-docs/version/1.2.4/reference/api/pandas.core.groupby.DataFrameGroupBy.count.html].\n",
    "\n",
    "When we're working with a `DataFrame`, all of our operations get applied individually to each row.  With `groupby` it's the same idea, except the operations are applied to **groups** of rows instead of individual rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59c7843",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['participant_id']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f12c37c",
   "metadata": {},
   "source": [
    "Are all of the entries equal to 25 as expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51d6991",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.groupby(['participant_id']).count() == 25).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c864ec56",
   "metadata": {},
   "source": [
    "We also expect that because we have 200 participants, we should have 200 observations for each menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5006288",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['menu_id']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3dc323",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.groupby(['menu_id']).count() == 200).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a452671",
   "metadata": {},
   "source": [
    "But having the right number of participants and right number of menus isn't quite enough - we also want that each participant should have seen each menu exactly once.  There are a few ways we can check for this.  One is by grouping jointly by `participant_id` and `menu_id`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159e328e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['participant_id', 'menu_id']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad462ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.groupby(['participant_id', 'menu_id']).count() == 1).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f14051",
   "metadata": {},
   "source": [
    "Another way is by using the function `duplicated`, which would return `True` for a row if it matches a previous row over all of the fields given. [https://pandas.pydata.org/pandas-docs/version/1.2.4/reference/api/pandas.Series.duplicated.html]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03bda6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated(['participant_id', 'menu_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d65d31",
   "metadata": {},
   "source": [
    "Here we would want to know if any of the entries in that `Series` were `True` - to do that we use the method `any`. [https://pandas.pydata.org/pandas-docs/version/1.2.4/reference/api/pandas.Series.any.html]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de108fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated(['participant_id', 'menu_id']).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369c7b35",
   "metadata": {},
   "source": [
    "That's a good place to save our work.  Remembering what we were saying before about always preserving the original data, we store the output of our work in a different directory.  I tend to use `prepared`, to indicate that it should be in a format that's ready for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca72011",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/prepared/decisions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1016ce",
   "metadata": {},
   "source": [
    "In this experiment there were four different treatments, which varied the information that participants were shown about the lotteries.  There is a separate `sessions` file which indicates which sessions were associated with which treatments.  We want to add this information into our table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09bca29",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = pd.read_csv(\"data/raw/sessions.csv\")\n",
    "sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b7efb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(sessions, how='left', on='session_id')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e109bb8e",
   "metadata": {},
   "source": [
    "We are expecting that participants were assigned in equal numbers to the four treatments.  Let us check this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75db497b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['treatment']).nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22a2332",
   "metadata": {},
   "source": [
    "Much of our focus in these few weeks is the process of going from raw, potentially messy source data to data which is ready for analysis.  This process, often called \"data wrangling\", will often take up most of your time in a real empirical project.  Ideally, by the time you start doing statistical or econometric analysis, the data are already in the format you need it, so your scripts (whether in Python or Stata or another package) that do the actual analysis will tend to be short - and short scripts are easier to read and understand!\n",
    "\n",
    "Having said that, after having done all of this work to prepare the data, I'm sure you're at least curious a bit about the results!  So let's look a little bit at some results - it'll also illustrate a few points about using `pandas` which we'll elaborate on in the next few weeks.\n",
    "\n",
    "First, the design of the experiment is that generally the lottery labeled 'p' has a higher expected value but also a higher variance (more risk) than the lottery 'q' in that menu.  Our hypothesis in the experiment is that giving information about expected value (treatments with E in the name) will increase the frequency with which lottery 'p' is chosen, while treatments giving information about risk (treatments with R in the name) will increase the frequency with which 'q' is chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111daf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['treatment', 'choice'])[['participant_id']].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3588b8",
   "metadata": {},
   "source": [
    "We can also view the 25 lotteries that a participant chooses over the experiment as them creating a 'portfolio'.  We can ask how much the portfolios differ across treatments.  To do this we'll use an auxiliary data file which contains the expected value and standard deviation of the lotteries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b85f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "lotteries = pd.read_csv(\"data/raw/lotteries.csv\")\n",
    "lotteries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02032458",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(\n",
    "    lotteries.rename(columns={'lottery': 'choice'}),\n",
    "    how='left', on=['menu_id', 'choice']\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783aa02e",
   "metadata": {},
   "source": [
    "In our experiment, we took the simplified view that all of the lottery outcomes were realised independently.  (In real finance applications they would be correlated, and correlation is an important part of portfolio design.  However for the research question for the experiment, the simpler environment of independence is useful.)\n",
    "\n",
    "Recall from your basic probability:\n",
    "1. The expected value of the 25 lotteries is the sum of the expected values of the lotteries (this does not depend on independence);\n",
    "2. The standard deviation of the 25 lotteries is the sum of the standard deviations of the lotteries (this does depend on independence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9534f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolios = (\n",
    "    df.groupby(['treatment', 'participant_id'])[['mean', 'stdev']].sum()\n",
    ")\n",
    "portfolios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48425623",
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolios = portfolios.reset_index()\n",
    "portfolios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69c9d69",
   "metadata": {},
   "source": [
    "Let's have a look at averages across the treatments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ff2af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolios.groupby('treatment')[['mean', 'stdev']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7065bd",
   "metadata": {},
   "source": [
    "We can do some quick visualisations to see whether there are really obvious patterns.  For this, we'll do some quick scatterplots using the `seaborn` library, which extends `matplotlib` by automating a number of processes around choosing axes, colours, legends, and so on.  We'll just have a look at what it can do here; we'll cover the library in more depth later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c283d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(\n",
    "    x='mean', y='stdev', hue='treatment',\n",
    "    data=portfolios.query(\"treatment.isin(['B', 'E'])\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3151839",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(\n",
    "    x='mean', y='stdev', hue='treatment',\n",
    "    data=portfolios.query(\"treatment.isin(['B', 'R'])\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fa6985",
   "metadata": {},
   "source": [
    "Let's now turn to the individual demographics data, which we will tidy up a bit.  These are in the files called `demographics`.  We'll take a bit of a shortcut and not check the schemas are the same (exercise: try it yourself!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b90305",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_demographics = pd.concat(\n",
    "    [pd.read_csv(\"data/raw/batch1/demographics.csv\"), pd.read_csv(\"data/raw/batch2/demographics.csv\")],\n",
    "    ignore_index=True\n",
    ")\n",
    "raw_demographics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9277363",
   "metadata": {},
   "source": [
    "Let's have a look at the columns and their data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc07aeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_demographics.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98382b91",
   "metadata": {},
   "source": [
    "In this case we know the fields we're particularly interested in: the seven \"about you\" questions.  Let's have a look at the data values for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10181aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_demographics[[\n",
    "    'player.gender', 'player.age', 'player.countryborn',\n",
    "    'player.countrynow', 'player.department', 'player.degree',\n",
    "    'player.timeuea'\n",
    "]].head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3c30fd",
   "metadata": {},
   "source": [
    "As before, we know we have entries for slots that were opened up for participants who did not turn up for the experiment.  However, trying to filter participants on whether or not demographics are null would be problematic, because participants cannot be obligated to disclose any or all of their demographic information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153441ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_demographics[[\n",
    "    'player.gender', 'player.age', 'player.countryborn',\n",
    "    'player.countrynow', 'player.department', 'player.degree',\n",
    "    'player.timeuea'\n",
    "]].query(\"`player.department`.isnull()\").head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3477f2f5",
   "metadata": {},
   "source": [
    "Here's where knowing how the software works is useful, including some of that data which is more about administering the experiment rather than collecting responses directly.  For each row there's a field called `participant._index_in_pages` (note the leading underscore), which tells you how far the participant has progressed in the experiment, and also `participant._max_page_index`, which is the total number of pages needed to complete the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf903d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_demographics[[\n",
    "    \"participant._index_in_pages\", \"participant._max_page_index\",\n",
    "    \"player.gender\", \"player.age\"\n",
    "]].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901b9dc9",
   "metadata": {},
   "source": [
    "The number of pages there are in a session depends on the session.  (As we were running the experiment, we realised having extra landing pages in the instructions was helpful to keep participants together.)  So the best way to test whether a participant row is a valid obseration is to see whether they reached the final page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2b165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_demographics.query(\"`participant._index_in_pages` == `participant._max_page_index`\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdf3752",
   "metadata": {},
   "source": [
    "We've got the right number of rows.  We'll check later on whether our participant IDs match up exactly with what we did with decisions.  For now, let's continue cleaning the data by selecting the columns we want (and while we're at it, let's get rid of those annoying periods in the column names)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177f27fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    df.rename(columns=lambda x: x.replace(\".\", \"_\"))\n",
    "    .reindex(\n",
    "        columns=['session_code', 'participant_code',\n",
    "                 'player_gender', 'player_age', 'player_countryborn',\n",
    "                 'player_countrynow', 'player_department', 'player_degree',\n",
    "                 'player_timeuea']\n",
    "\n",
    "    )\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec94a2bc",
   "metadata": {},
   "source": [
    "Sometimes, there are fields where there is a finite list of possible answers - but that list is too long to specify completely in a question.  Countries are a good example of this; there are roughly 200 in the world (depending on how you count), but we're all had the experience of how tedious it is to pick out your country from a long drop-down list.  We have two country fields in our data; this is a good opportunity to look at ways to tidy up the data.\n",
    "\n",
    "Let's first look at `countryborn`, and see what data values are there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3441de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('player_countryborn')['player_countryborn'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf08c9e",
   "metadata": {},
   "source": [
    "Compared to some datasets, this isn't all that bad; most of the country names are already rather clean.  We just need to standardise a few country names, and to make a decision about how to code situations where more than one country is listed.\n",
    "\n",
    "To accomplish this we'll use two functions:\n",
    "1. `Series.str.title()`: This will convert all the strings to Title Case - that is, first letter of each work capitalised and all others lowercase; [https://pandas.pydata.org/pandas-docs/version/1.2.4/reference/api/pandas.Series.str.title.html]\n",
    "2. `Series.replace(): This takes a `dict`, and replaces each instance of a key with the corresponding value in the `dict`. [https://pandas.pydata.org/pandas-docs/version/1.2.4/reference/api/pandas.Series.replace.html]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e9a8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.assign(\n",
    "    player_countryborn = lambda x: (\n",
    "        x['player_countryborn'].str.title()\n",
    "        .replace(\n",
    "            {'Britain': 'United Kingdom',\n",
    "             'British': 'United Kingdom',\n",
    "             'Uk': 'United Kingdom',\n",
    "             'Uk (England)': 'United Kingdom',\n",
    "             'Uk, Australia': 'United Kingdom',\n",
    "             'England': 'United Kingdom',\n",
    "             'England / Uk': 'United Kingdom',\n",
    "             'United Kingdom (England)': 'United Kingdom',\n",
    "             'Denmark/Usa': 'Denmark',\n",
    "             'United States Of America': 'United States',\n",
    "             'Usa': 'United States',\n",
    "             'Taiwan, Egypt': 'Taiwan'\n",
    "            }\n",
    "        )\n",
    "    )\n",
    ")\n",
    "df.sort_values('player_countryborn')['player_countryborn'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a70a67c",
   "metadata": {},
   "source": [
    "Looks good.  Now we'll do the same exercise with `player_countrynow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eb829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('player_countrynow')['player_countrynow'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26eabfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.assign(\n",
    "    player_countrynow = lambda x: (\n",
    "        x['player_countrynow'].str.title()\n",
    "        .replace(\n",
    "            {'Britain': 'United Kingdom',\n",
    "             'British': 'United Kingdom',\n",
    "             'Uk': 'United Kingdom',\n",
    "             'Uk (England)': 'United Kingdom',\n",
    "             'Uk England': 'United Kingdom',\n",
    "             'Uk, Australia': 'United Kingdom',\n",
    "             'England, Uk': 'United Kingdom',\n",
    "             'England': 'United Kingdom',\n",
    "             'England / Uk': 'United Kingdom',\n",
    "             'Uk As A Visiting Student': 'United Kingdom',\n",
    "             'United Kingdom (England)': 'United Kingdom',\n",
    "             'Denmark/Usa': 'Denmark',\n",
    "             'United States Of America': 'United States',\n",
    "             'Usa': 'United States',\n",
    "             'Taiwan, Egypt': 'Taiwan'\n",
    "            }\n",
    "        )\n",
    "    )\n",
    ")\n",
    "df.sort_values('player_countrynow')['player_countrynow'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839dc9ea",
   "metadata": {},
   "source": [
    "Now let's have a look at `player_gender`.  This is an example of a quite annoying data field - the data are recorded by the computer as integers, but you have to know the computer code to know what is what.  Because we do have the computer code, we know that 1 = Male, 2 = Female, 3 = Other, and 4 = prefer not to say.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51b7400",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('player_gender')['participant_code'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5f8503",
   "metadata": {},
   "source": [
    "We'll recode these using letters (M, F, O), and replace 4 with true null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641853f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.assign(\n",
    "    player_gender = lambda x: (\n",
    "        x['player_gender'].replace(\n",
    "            {1: 'M',\n",
    "             2: 'F',\n",
    "             3: 'O',\n",
    "             4: None}\n",
    "        )\n",
    "    )\n",
    ")\n",
    "df.groupby('player_gender')['participant_code'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baadbee",
   "metadata": {},
   "source": [
    "Now, let's have a look at the responses for UEA schools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b40a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('player_department')['player_department'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f8e9bb",
   "metadata": {},
   "source": [
    "These aren't too bad.  To clean these up, alongside `Series.replace` which we've already used, we'll make use of two useful methods for string manipulation:\n",
    "\n",
    "1. `Series.str.upper()`: Converts all characters in the string to uppercase. [https://pandas.pydata.org/pandas-docs/version/1.2.4/reference/api/pandas.Series.str.upper.html]\n",
    "2. `Series.str[]`: The `[]` notation on a Series works just like it does on regular Python strings or lists.  We'll use it here to restrict to the first three letters - which, after a bit of initial cleanup, maps to the UEA School/Faculty/programme names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e36293d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.assign(\n",
    "    player_department=lambda x: (\n",
    "        x['player_department'].replace(\n",
    "            {\"BSc Psychology\": \"PSY\",\n",
    "             \"NMS\": \"MED\",\n",
    "             \"UEA\": None}\n",
    "        )\n",
    "        .str.upper()\n",
    "        .str[:3]\n",
    "    )\n",
    ")\n",
    "df.sort_values('player_department')['player_department'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a289f3af",
   "metadata": {},
   "source": [
    "The UEA degree/affiliation field is, like gender, straightforward enough if you have the coding from the software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d119286",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.assign(\n",
    "    player_degree=lambda x: (\n",
    "        x['player_degree'].replace(\n",
    "            {1: \"INTO\",\n",
    "             2: \"BSc\",\n",
    "             3: \"PGDip\",\n",
    "             4: \"MA/MSc\",\n",
    "             5: \"PhD\",\n",
    "             6: \"Staff\",\n",
    "             7: \"Other\",\n",
    "             8: None}\n",
    "        )        \n",
    "    )\n",
    ")\n",
    "df.sort_values('player_degree')['player_degree'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f20ecc",
   "metadata": {},
   "source": [
    "Likewise, coding up the time-at-UEA question is now routine (I hope!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf1aafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.assign(\n",
    "    player_timeuea=lambda x: (\n",
    "        x['player_timeuea'].replace(\n",
    "            {1: \"1st\",\n",
    "             2: \"2nd\",\n",
    "             3: \"3rd\",\n",
    "             4: \"4th\",\n",
    "             5: \"5th+\",\n",
    "             6: None}\n",
    "        )\n",
    "    )\n",
    ")\n",
    "df.sort_values('player_timeuea')['player_timeuea'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ac6892",
   "metadata": {},
   "source": [
    "Let's take stock of where we are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efc9e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ee0adb",
   "metadata": {},
   "source": [
    "We're rather close; just a few further adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6821cd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    df.rename(columns=lambda x: x.replace(\"player_\", \"\"))\n",
    "    .rename(columns={\n",
    "        'session_code': 'session_id',\n",
    "        'participant_code': 'participant_id'\n",
    "    })\n",
    "    .astype({'age': int})\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b489dc",
   "metadata": {},
   "source": [
    "We'll save our work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60bf7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/prepared/demographics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22980b0d",
   "metadata": {},
   "source": [
    "As a final exercise, let's do a quick look at demographics and decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eefe168",
   "metadata": {},
   "outputs": [],
   "source": [
    "decisions = pd.read_csv(\"data/prepared/decisions.csv\")\n",
    "decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28a13db",
   "metadata": {},
   "source": [
    "Let's check to confirm our list of participant IDs do match across the two files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f147185",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('participant_id')['participant_id'].unique() == decisions.sort_values('participant_id')['participant_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be2475a",
   "metadata": {},
   "outputs": [],
   "source": [
    "decisions = decisions.groupby(['participant_id', 'choice'])[['menu_id']].count()\n",
    "decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fa7d8d",
   "metadata": {},
   "source": [
    "Let's just look at how many choices of 'p' each participant made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abc487f",
   "metadata": {},
   "outputs": [],
   "source": [
    "decisions = decisions.reset_index()\n",
    "decisions = decisions.query(\"choice == 'p'\")\n",
    "decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd716450",
   "metadata": {},
   "source": [
    "Simple thing we might look at: Is there a difference in gender with respect to the frequency of choices of 'p'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8210470",
   "metadata": {},
   "outputs": [],
   "source": [
    "decisions = decisions.merge(df, how='left', on='participant_id')\n",
    "decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b4452b",
   "metadata": {},
   "outputs": [],
   "source": [
    "decisions.groupby('gender')[['menu_id']].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
