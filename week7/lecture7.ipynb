{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dd3b7ed",
   "metadata": {},
   "source": [
    "As a further exercise, we'll now clean and prepare the data from another behavioural economics paper, \"Honesty, beliefs about honesty, and economic growth in 15 countries\" (Journal of Economic Behavior & Organization 2016). This reports an online experiment.\n",
    "\n",
    "The experiment data came from Qualtrics as an Excel file. It is quite messy.  In particular, Qualtrics (and Google Forms, and Office 365 Forms) store the responses to questions in columns where the column name is the text of the question.\n",
    "\n",
    "**Question**: Is it good or bad that the software uses the question name text as variable names?\n",
    "\n",
    "As we have seen in our previous exercises, data cleaning involves a standard set of common tasks:\n",
    "\n",
    "* Importing data from external files\n",
    "* \"Eyeballing\" the data manually\n",
    "* Removing unwanted rows\n",
    "* Removing unwanted columns\n",
    "* Renaming variables\n",
    "* Recoding variables, into a new or the same variable\n",
    "* Merging different datasets, by row or by column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37904a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ac4304",
   "metadata": {},
   "outputs": [],
   "source": [
    "hon_data = pd.read_excel(\"data/honesty-data.xlsx\")\n",
    "hon_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dba376",
   "metadata": {},
   "source": [
    "From the 2016 paper:\n",
    "\n",
    "*People from 15 countries took part in an online survey containing two incentivized experiments measuring honest behaviour. I use both the well-known coin flip experiment, where subjects report the result of a coin flip and are offered money for reporting \"heads\", and a new experimental paradigm: an online quiz in which subjects were able to cheat and this could be detected.*\n",
    "\n",
    "To reanalyze this data we will need to know what country subjects were from; whether they reported heads in the coin flip; and what their quiz answers were.\n",
    "\n",
    "Let's look at the column names of the data.  We have already seen how to do this with `.columns`; another way to get a nice list of column details is with the method `.info()`.  This method also has the benefit of telling us how many non-null entries are in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd518d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hon_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6067bfc5",
   "metadata": {},
   "source": [
    "This looks encouraging. We see that column 33 (remember to count from zero!) is called \"Did the coin land on heads?\" and column 51 is \"Please enter your nationality:\" There are also some quiz questions in columns 22-28.\n",
    "\n",
    "Let's look at column 33. We hope there will be two values in it, representing heads and tails:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a2c866",
   "metadata": {},
   "outputs": [],
   "source": [
    "hon_data[\"Did the coin land on heads?\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e3a5b3",
   "metadata": {},
   "source": [
    "We have some good and bad news. Good news: we can see 1 and 2. Those probably mean heads and tails. Bad news: there is some other stuff in there - a question number, the question text itself, and `nan` which means \"not a number\".  We now need to figure out where this other data comes from. \n",
    "\n",
    "First we can check for how many rows have a value which is equal to 1 or 2.  Two notes:\n",
    "\n",
    "* The function `.isin()` returns a `Series` whose elements are type `bool`.  To negate a Boolean series, we use the tilde operator `~`.\n",
    "* Here we assign the result to a `Series` that is not a column in the `DataFrame`.  We don't necessarily have to store the results of every calculation on `DataFrame` columns in the `DataFrame` itself.  In this case, our `not_1_or_2` is an auxiliary calculation we are doing.  We don't need to keep it long-term, so there's no need to assign it to a column in the `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a92d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_1_or_2 = hon_data[\"Did the coin land on heads?\"].isin([1,2])\n",
    "not_1_or_2 = ~is_1_or_2\n",
    "not_1_or_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dd025b",
   "metadata": {},
   "source": [
    "That looks OK - most of the rows seem to be 1 or 2 (so `not_1_or_2` is False). We can check by counting the rows. To do this we just take the sum of `not_1_or_2`. This trick uses the useful fact that if you add boolean values in python, they get treated as 1 for True and 0 for False:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8999a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "True + True + False + True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8630b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(not_1_or_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8ffe41",
   "metadata": {},
   "source": [
    "That seems like a lot of dodgy rows.  Looking at the dataset manually in Excel, we find:\n",
    "\n",
    "* Many rows don't contain any data beyond the first few columns. These might be participants who gave up early, or who failed a check to qualify for the survey.\n",
    "* The first row contains the question (e.g. \"Did the coin land on heads?\")\n",
    "* Rows 322 and 323 repeat the question, along with a list of question numbers.\n",
    "* Lastly, some rows are blank in column 33 - perhaps because the participant simply didn't answer.\n",
    "\n",
    "None of these rows contain the data we need, so we can safely delete them.\n",
    "\n",
    "In practice, it would always be a good idea to know as much you can about how the data were collected.  For example, if you got this data from someone else, you might ask them why there are so many rows which do not seem to have valid data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92abc374",
   "metadata": {},
   "source": [
    "We know how to use `.query()` to select rows from a `DataFrame`.  We could use this to get the rows we are interested in by doing\n",
    "\n",
    "```hon_data.query(\"`Did the coin land on heads?`.isin([1,2])\")```\n",
    "\n",
    "But, `query()` has a feature that allows us to use the work we have already done.  If you put an `@` in front of a variable, `query()` looks at the variables defined in your current scope in Python and uses those.  So we can use our existing `is_1_or_2` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ea1187",
   "metadata": {},
   "outputs": [],
   "source": [
    "hon_data = hon_data.query(\"@is_1_or_2\")\n",
    "hon_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb37f8f",
   "metadata": {},
   "source": [
    "At this point, the horrible column names are starting to give me a headache.  And, there are a lot of columns that aren't relevant to our objectives.  (As when doing a maths problem, what you need to do in developing your recipes for cleaning up data depends on what you are trying to accomplish.  'The wise person begins at the end; the foolish person ends at the beginning.')\n",
    "\n",
    "Let's make some progress towards rationalising the data by removing the \"block randomizer\" columns.  First we need to make a list of the matching column names.  Here is one way to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fee7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_rand = []\n",
    "for c in hon_data.columns:\n",
    "    if \"Block Randomizer\" in c:\n",
    "        block_rand.append(c)\n",
    "block_rand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0398f8c6",
   "metadata": {},
   "source": [
    "However - this is a very common pattern in programming, and perhaps not surprisingly Python has a more compact way of writing this using **list comprehensions**.\n",
    "\n",
    "In a list comprehension, one can put the for-loop inside the definition of the list you want to make.  The resulting code is... Pythonic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53c7775",
   "metadata": {},
   "outputs": [],
   "source": [
    "[c for c in hon_data.columns if \"Block Randomizer\" in c]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7b68dd",
   "metadata": {},
   "source": [
    "We can then write our `drop` expression quite elegantly, in a way that (we hope) makes it clear what our intention is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a4d3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hon_data = hon_data.drop(columns=[c for c in hon_data.columns if \"Block Randomizer\" in c])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a32b424",
   "metadata": {},
   "source": [
    "There are a few columns which we definitely will want to retain; let's give them nicer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45073c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hon_data = hon_data.rename(columns={\n",
    "    \"Did the coin land on heads?\": \"heads\",\n",
    "    \"What is your gender?\": \"gender\",\n",
    "    \"How old are you?\": \"age\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a9d463",
   "metadata": {},
   "source": [
    "Several of the columns contain whether or not the participant got the answer to quiz questions correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4310f597",
   "metadata": {},
   "outputs": [],
   "source": [
    "[c for c in hon_data.columns if \"correct\" in c]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96c079e",
   "metadata": {},
   "source": [
    "We've now many times seen the benefits of having column names which are also valid Python function or variable names.  Let's clean these!\n",
    "\n",
    "First, let's do this by defining a bespoke function that takes a column name `x` and makes it lowercase, with underscores instead of spaces, if the column name starts with \"Quiz\" and ends with \"correct\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef682c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_quiz_column(x):\n",
    "    if x.startswith(\"Quiz\") and x.endswith(\"correct\"):\n",
    "        return x.lower().replace(\" \", \"_\")\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "print(\n",
    "    hon_data.rename(columns=rename_quiz_column).columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea4b038",
   "metadata": {},
   "source": [
    "Could we do this more compactly by using a `lambda` function instead of one we define via `def`?  Your first thought might be that `rename_quiz_column` looks like it has more than one expression, so we can't write it as a `lambda`.\n",
    "\n",
    "However, much as we can embed a `for` loop in a list comprehension, Python also allows us to do **conditional expressions**, which let us put `if`/`else` logic in a single expression.  With that we can in fact do the renaming with a `lambda`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41eff798",
   "metadata": {},
   "outputs": [],
   "source": [
    "hon_data = hon_data.rename(\n",
    "    columns=lambda x: x.lower().replace(\" \", \"_\") if x.startswith(\"Quiz\") and x.endswith(\"correct\") else x\n",
    ")\n",
    "print(hon_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cd4ce3",
   "metadata": {},
   "source": [
    "Which one is better - the `def` or the `lambda`?  It depends on the situation; next week when we talk about organising one's work into scripts, we'll see that perhaps in this case putting the logic in a `def` could actually be the more transparent way to write it.  But neither is wrong; it's a question of personal preference and style, not correctness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961ce533",
   "metadata": {},
   "source": [
    "The `nationality` variable records partcipants' answers to \"What nationality are you?\" They were given as free text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0a3880",
   "metadata": {},
   "outputs": [],
   "source": [
    "hon_data['nationality'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e807b0",
   "metadata": {},
   "source": [
    "We want these names to be consistent across subjects, so that e.g. \"turkey\", \"Turkey\" and \"Türk\" are the same.\n",
    "\n",
    "As a start, we can make all the nationalities lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabd95c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hon_data = hon_data.assign(**{\n",
    "    'nationality': lambda x: x['nationality'].str.lower()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a7627e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hon_data['nationality'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976787ef",
   "metadata": {},
   "source": [
    "We can then standardise these to country names.  In the case of \"t.c.\", it might not be commonly-known what this is an abbreviation for.  In such a case, if you have to look something up, it's probably a good idea to include a comment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa553f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "hon_data = hon_data.assign(**{\n",
    "    'nationality': lambda x: x['nationality'].replace({\n",
    "        \"türk\"      : \"turkey\",\n",
    "        \"turkey\"    : \"turkey\",\n",
    "        \"t.c.\"      : \"turkey\", # Wikipedia: short for \"Türkiye Cumhuriyeti\".\n",
    "        \"tc\"        : \"turkey\",\n",
    "        \"polish\"    : \"poland\",\n",
    "        \"american\"  : \"US\",\n",
    "        \"german\"    : \"germany\",\n",
    "        \"russian\"   : \"russia\",\n",
    "        \"italian\"   : \"italy\",\n",
    "        \"australian\": \"australia\",\n",
    "        \"irish\"     : \"ireland\",\n",
    "        \"japanese\"  : \"japan\",\n",
    "        \"malaysian\" : \"malaysia\",\n",
    "        \"chinese\"   : \"china\",\n",
    "        \"indian\"    : \"india\",\n",
    "        \"togolese\"  : \"togo\",\n",
    "        \"swiss\"     : \"switzerland\",\n",
    "        \"brazilian\" : \"brazil\",\n",
    "        \"guinean\"   : \"guinea\",\n",
    "    })\n",
    "})\n",
    "hon_data[\"nationality\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13b0fb0",
   "metadata": {},
   "source": [
    "There are still some participants we haven't dealt with - those who gave a nationality like \"white\" or \"musulman\", or a mixed nationality like \"italian/irish\". We can't categorize those into one of my national groups, so in this instance we are going to delete them.\n",
    "\n",
    "Also, there are some people who are not in the group of nations selected for the survey. For example, there is one Romanian. We only want the 8 nations targeted in (this part of) the survey.\n",
    "\n",
    "⚠️ **Choices made during data cleaning can have statistical implications!** When we delete people who describe their nationality as \"white\" or \"musulman\", I may be selecting out subjects with a particular sense of national identity! Similarly, by excluding people who were resident in one of these 8 countries, but from a different nation, I exclude migrants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec626da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hon_data = hon_data.query(\n",
    "    'nationality.isin([\"US\", \"brazil\", \"russia\", \"turkey\", \"china\", \"japan\", \"greece\", \"switzerland\"])'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a33a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "hon_data['nationality'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9660fe52",
   "metadata": {},
   "source": [
    "Because these are country names, let's capitalise them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5235d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "hon_data = hon_data.assign(**{\n",
    "    'nationality': lambda x: x['nationality'].str.capitalize()\n",
    "})\n",
    "hon_data['nationality'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4adb46d",
   "metadata": {},
   "source": [
    "Ahh, one last adjustment; we want \"US\" instead of \"Us\"...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f5a44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hon_data = hon_data.assign(**{\n",
    "    'nationality': lambda x: x['nationality'].replace(\"Us\", \"US\")\n",
    "})\n",
    "hon_data['nationality'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18b12c8",
   "metadata": {},
   "source": [
    "Now, let's have a look at some of the core \"honesty\" data - which is whether the participant reported the coin toss to be heads or tails.  The coding of this field is ambiguous, but perhaps we can figure out how it is coded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365b018e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hon_data['heads'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e10a78",
   "metadata": {},
   "source": [
    "Because in this experiment there were incentives to say the coin toss was heads, it's quite probable that this implies that 1 = heads and 2 = tails - this is certainly the finding in many other experiments using the same instrument.  So let's re-code that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b68f2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hon_data = hon_data.assign(**{\n",
    "    'heads': lambda x: x['heads'] == 1\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb490d1",
   "metadata": {},
   "source": [
    "Some of the columns are results from an \"integrity test\" where participants were asked whether they thought certain actions were always (1), sometimes (2), rarely (3), or never (4) justified.\n",
    "\n",
    "These are the columns that start with \"Please / think\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e796a55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "[c for c in hon_data.columns if c.startswith(\"Please / think\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc91a97",
   "metadata": {},
   "source": [
    "We'd like to create a single total score from these columns.  Higher scores will indicate greater \"integrity.\"\n",
    "\n",
    "When we did something similar previously (with numeracy scores), we saw that a wide-to-long transformation can be an attractive way to do this kind of calculation.  However, we can also use the `sum` method.  The default for `sum` is to sum *down* a column (as we have seen before).  If we specify the `axis` parameter, we can instead sum *across* columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d061b7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "integrity = hon_data[[c for c in hon_data.columns if c.startswith(\"Please / think\")]].sum(axis='columns')\n",
    "integrity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b556e7aa",
   "metadata": {},
   "source": [
    "Let's have a quick look at the distribution of these scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f1a852",
   "metadata": {},
   "outputs": [],
   "source": [
    "integrity.value_counts().sort_index().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6433a111",
   "metadata": {},
   "source": [
    "The distribution is quite skewed.  We might want to bin this data for analysis.  We can do this using the `cut()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65e3b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pd.cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2e9570",
   "metadata": {},
   "outputs": [],
   "source": [
    "hon_data = hon_data.assign(**{\n",
    "    'integrity': pd.cut(integrity, [0, 45, 50, 55, 60])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1afe0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "hon_data['integrity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf463b30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
